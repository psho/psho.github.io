{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else\n",
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = '/data-directory/'\n",
    "df = pd.read_csv(IN_PATH + 'file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "len(df)\n",
    "\n",
    "# Sample\n",
    "df.head(10)\n",
    "\n",
    "# Summary stats\n",
    "df.describe(include='all')\n",
    "\n",
    "# Categories\n",
    "df['catvar'].astype('category').cat.categories\n",
    "\n",
    "# Missings\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df['date'] = pd.to_datetime(df['date_string'], format='%d%b%Y')\n",
    "\n",
    "# Days between\n",
    "df['days_between'] = (df['date1'] - df['date2']).dt.days\n",
    "\n",
    "# Substring before first digit\n",
    "df['substring'] = df['string'].str.split(r'\\d').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign categorical vs continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['cat_var1', 'cat_var2', 'cat_var3']\n",
    "\n",
    "con_vars = ['con_var1', 'con_var2', 'con_var3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale continuous values\n",
    "\n",
    "If there's a separate test set, remember to save the mapper so you can apply the exact same transformation on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance of each continuous variable\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[con_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise each continuous variable by subtracting mean and dividing by stddev\n",
    "df[con_vars] = scaler.transform(df[con_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[con_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map categorical values to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast categorical variables to the correct type\n",
    "for c in cat_vars:\n",
    "    df[c] = df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categorical values to integers (leave 0 for missing)\n",
    "for col_name, col_val in df[cat_vars].items():\n",
    "    df[col_name] = col_val.cat.codes+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up entity embeddings for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sz = [(c, len(df[c].cat.categories)+1) for c in cat_vars]\n",
    "cat_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out data for training, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly allocate row numbers for training, validation and test\n",
    "np.random.seed(123)\n",
    "valid_pct = 0.2\n",
    "test_pct = 0.2\n",
    "\n",
    "n = len(df)\n",
    "n_valid = int(valid_pct * n)\n",
    "n_test = int(test_pct * n)\n",
    "\n",
    "row_nums = np.random.permutation(n)\n",
    "\n",
    "valid_rows = row_nums[:n_valid]\n",
    "test_rows  = row_nums[n_valid:n_valid+n_test]\n",
    "train_rows = row_nums[n_valid+n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_train = df.iloc[train_rows]\n",
    "df_valid = df.iloc[valid_rows]\n",
    "df_test  = df.iloc[test_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out target variables\n",
    "y_train = df_train['target']\n",
    "x_train = df_train.drop(['target'], axis=1)\n",
    "\n",
    "y_valid = df_valid['target']\n",
    "x_valid = df_valid.drop(['target'], axis=1)\n",
    "\n",
    "y_test  = df_test['target']\n",
    "x_test  = df_test.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target log instead of raw value\n",
    "log_y_train = np.log(y_train)\n",
    "log_y_valid = np.log(y_valid)\n",
    "log_y_test  = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap range of valid values\n",
    "max_log_y_train = np.max(log_y_train)\n",
    "log_y_train_range = (0, max_log_y_train*1.2)\n",
    "log_y_train_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up embedding dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    # List of categorical and continuous variables\n",
    "    def __init__(self, df_cat, df_con, target):\n",
    "\n",
    "        # Get series values for each cat/con variable\n",
    "        cat_vals = [c.values for n,c in df_cat.items()]\n",
    "        con_vals = [c.values for n,c in df_con.items()]\n",
    "\n",
    "        # Stack all\n",
    "        self.cats = np.stack(cat_vals, 1).astype(np.int64)\n",
    "        self.cons = np.stack(con_vals, 1).astype(np.float32)\n",
    "        self.target = target[:, None].astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.cons[idx], self.target[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmbeddingDataset(x_train[cat_vars], x_train[con_vars], log_y_train)\n",
    "valid_data = EmbeddingDataset(x_valid[cat_vars], x_valid[con_vars], log_y_valid)\n",
    "test_data  = EmbeddingDataset(x_test[cat_vars], x_test[con_vars], log_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "train_data_loader = DataLoader(train_data, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, emb_szs, n_con, target_range):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        # Hold all embedding submodules in a list\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c, s in emb_szs])\n",
    "\n",
    "        # Initialise embeddings with some \"reasonable\" values based on some rule-of-thumb process\n",
    "        for emb in self.embs:\n",
    "            sc = 2 / (emb.weight.data.size(1) + 1)\n",
    "            emb.weight.data.uniform_(-sc, sc)\n",
    "\n",
    "        # Define layers\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(0.04)\n",
    "        self.bn = nn.BatchNorm1d(n_con)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_emb + n_con, 1000)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight.data)\n",
    "\n",
    "        self.dr1 = nn.Dropout(0.001)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight.data)\n",
    "\n",
    "        self.dr2 = nn.Dropout(0.01)\n",
    "\n",
    "        self.outp = nn.Linear(500, 1)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "\n",
    "        self.target_range = target_range\n",
    "    \n",
    "    def forward(self, x_cat, x_con):\n",
    "      \n",
    "        # Embeddings\n",
    "        x = [e(x_cat[:, i]) for i,e in enumerate(self.embs)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "\n",
    "        # Continuous\n",
    "        x2 = self.bn(x_con)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "\n",
    "        # Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dr2(x)\n",
    "        x = self.outp(x)\n",
    "\n",
    "        # Output\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x * (self.target_range[1] - self.target_range[0])\n",
    "        x = x + self.target_range[0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNet(emb_szs, n_con=len(con_vars), target_range=log_y_train_range)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find good learning rate to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpRangeLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, batches_per_epoch, lr_start=1e-5, lr_end=10, last_epoch=-1):\n",
    "        self.lr_start = lr_start\n",
    "        self.lr_mult = (lr_end/lr_start)**(1/batches_per_epoch)\n",
    "        self.lr_hist = []  # track history of learning rates\n",
    "\n",
    "        super(ExpRangeLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = [self.lr_start * (self.lr_mult**self.last_epoch)\n",
    "               for base_lr in self.base_lrs]\n",
    "\n",
    "        self.lr_hist.append(lrs)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, data_loader, optimizer, scheduler, criterion):\n",
    "    i = 0\n",
    "    best_loss = -1\n",
    "\n",
    "    avg_loss = 0\n",
    "    raw_loss_hist = []\n",
    "    debias_loss_hist = []\n",
    "\n",
    "    for d in iter(data_loader):\n",
    "        i = i + 1\n",
    "\n",
    "        # Set learning rate for this batch (first step = starting LR)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Split into categorical, continuous, and target variables\n",
    "        x_cats, x_cons, y_act = d\n",
    "\n",
    "        # Move to device\n",
    "        x_cats = x_cats.to(device)\n",
    "        x_cons = x_cons.to(device)\n",
    "        y_act = y_act.to(device)\n",
    "\n",
    "        # Forward pass: Compute predictions with current parameters and calculate loss\n",
    "        y_pred = model(x_cats, x_cons)\n",
    "        loss = criterion(y_pred, y_act)\n",
    "\n",
    "        raw_loss = loss.item()\n",
    "        avg_loss = (avg_loss * 0.98) + (raw_loss * 0.02)\n",
    "        debias_loss = avg_loss / (1-(0.98**i))\n",
    "\n",
    "        # Keep track of the losses\n",
    "        raw_loss_hist.append(raw_loss)\n",
    "        debias_loss_hist.append(debias_loss)\n",
    "\n",
    "        # Stop if loss starts increasing\n",
    "        if best_loss == -1 or debias_loss < best_loss:\n",
    "            best_loss = debias_loss\n",
    "\n",
    "        if debias_loss > best_loss * 4:\n",
    "            break\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: Gradient of loss wrt to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "  \n",
    "        # Return learning rates and losses\n",
    "        lr_hist = [lr[0] for lr in scheduler.lr_hist[1:]]\n",
    "        return lr_hist, raw_loss_hist, debias_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_start = 1e-5\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr_start)\n",
    "crit = F.mse_loss\n",
    "batches_per_epoch = math.ceil(len(train_data)/bs)\n",
    "sched = ExpRangeLR(optimizer=opt, batches_per_epoch=batches_per_epoch, lr_start=lr_start, lr_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hist, raw_loss_hist, debias_loss_hist = find_lr(model=model, data_loader=train_data_loader, optimizer=opt, scheduler=sched, criterion=crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_batch = math.floor(batches_per_epoch*0.05)  # Skip first 5% as loss rapidly improves\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.xscale('log')\n",
    "plt.plot(lr_hist[start_batch:], debias_loss_hist[start_batch:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up custom learning rate scheduler - Cosine Annealing with Restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: _LRScheduler class assumes step is taken at each epoch, but we step at each minibatch. So imagine epoch==minibatch here.\n",
    "# NOTE: _LRScheduler class takes a step upon init\n",
    "class CosineAnnealingLRWithRestarts(_LRScheduler):\n",
    "  \n",
    "    def __init__(self, optimizer, T_max, cycle_mult=1, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max  # number of epochs in a cycle\n",
    "        self.eta_min = eta_min\n",
    "\n",
    "        self.cycle_mult = cycle_mult  # multiplying factor for next cycle length\n",
    "        self.cycle_len = self.T_max  # number of epochs in first cycle\n",
    "        self.curr_epoch = last_epoch\n",
    "        self.lr_hist = []  # track history of learning rates\n",
    "\n",
    "        super(CosineAnnealingLRWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
    "               (1 + math.cos(math.pi * self.curr_epoch / self.cycle_len)) / 2\n",
    "               for base_lr in self.base_lrs]\n",
    "\n",
    "        self.lr_hist.append(lrs)\n",
    "        return lrs\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Restart\n",
    "        self.curr_epoch += 1\n",
    "        if self.curr_epoch == self.cycle_len:\n",
    "            self.curr_epoch = 0\n",
    "            self.cycle_len = int(self.cycle_len * self.cycle_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_embeddings(model, train_dl, valid_data, optimizer, scheduler, criterion, num_cycles=1, cycle_mult=1):\n",
    "  \n",
    "    # Print output heading\n",
    "    heading = 'Epoch   Train_Loss  Valid_Loss'\n",
    "    print(heading)\n",
    "    print('-' * len(heading))\n",
    "\n",
    "    # Load validation data\n",
    "    xv_cats = torch.from_numpy(valid_data.cats).to(device)\n",
    "    xv_cons = torch.from_numpy(valid_data.cons).to(device)\n",
    "    yv_act = torch.from_numpy(valid_data.target).to(device)\n",
    "\n",
    "    # Calculate total number of epochs required\n",
    "    epochs = math.ceil((1-cycle_mult**num_cycles)/(1-cycle_mult)) if cycle_mult>1 else num_cycles\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for d in iter(train_dl):\n",
    "\n",
    "            # Set learning rate for this batch (first step = starting LR)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Split into categorical, continuous, and target variables\n",
    "            x_cats, x_cons, y_act = d\n",
    "\n",
    "            # Move to device\n",
    "            x_cats = x_cats.to(device)\n",
    "            x_cons = x_cons.to(device)\n",
    "            y_act = y_act.to(device)\n",
    "\n",
    "            # Forward pass: Compute predictions with current parameters and calculate loss\n",
    "            y_pred = model(x_cats, x_cons)\n",
    "            loss = criterion(y_pred, y_act)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: Gradient of loss wrt to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "    # Score validation data\n",
    "    yv_pred = model(xv_cats, xv_cons)\n",
    "    valid_loss = criterion(yv_pred, yv_act)\n",
    "    print('{:<8d}{:<12.4f}{:<12.4f}'.format(e+1, loss.item(), valid_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise model after updating weights from finding the LR\n",
    "model = EmbeddingNet(emb_szs, n_con=len(con_vars), target_range=log_y_train_range)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "crit = F.mse_loss\n",
    "batches_per_epoch = math.ceil(len(train_data)/bs)\n",
    "\n",
    "num_cycles = 5\n",
    "cycle_mult = 1\n",
    "sched = CosineAnnealingLRWithRestarts(optimizer=opt, T_max=batches_per_epoch, cycle_mult=cycle_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_embeddings(model=model, train_dl=train_data_loader, valid_data=valid_data, optimizer=opt, scheduler=sched, criterion=crit, num_cycles=num_cycles, cycle_mult=cycle_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var1_embedding = model.embs[2].weight.detach().numpy()\n",
    "\n",
    "labels = df['cat_var1'].astype('category').cat.categories\n",
    "# OR\n",
    "labels = ['N/A','Cat1','Cat2','Cat3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(cat_var1_embedding)\n",
    "dendrogram(Z, orientation='left', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(cat_var1_embedding)\n",
    "df_cluster = pd.DataFrame({'Label': labels,\n",
    "                          'Cluster': kmeans.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['Cluster']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['Cluster']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['Cluster']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(init='pca', random_state=0, method='exact')\n",
    "Y = tsne.fit_transform(cat_var1_embedding)\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.scatter(-Y[:, 0], -Y[:, 1])\n",
    "for i, txt in enumerate(labels):\n",
    "    plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nikola": {
   "category": "",
   "date": "2019-04-13 19:36:07 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "pytorch-dl-tabular-template",
   "tags": "",
   "title": "PyTorch: Deep Learning on Tabular Data - Template",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
