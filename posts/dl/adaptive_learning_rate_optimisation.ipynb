{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([14,86,28,51,28,29,72,62,84,15,\n",
    "              42,62,47,35,9,38,44,99,13,21,\n",
    "              28,20,8,64,99,70,27,17,8])\n",
    "\n",
    "B = 30\n",
    "W = 2\n",
    "\n",
    "y = B + W*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Loss function is square error:\n",
    "\n",
    "$$loss=(b+wx-y)^2$$\n",
    "\n",
    "Gradients by chain rule:\n",
    "\n",
    "$$F'(g(x)) = f’(g(x))g’(x)$$\n",
    "\n",
    "$$\\frac{dl}{db} = 2(b+wx-y)$$\n",
    "\n",
    "$$\\frac{dl}{dw} = 2(b+wx-y)*x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 0\n",
    "w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 2.83 (actual 30.00)\n",
      "w = 2.58 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "        \n",
    "        # Update parameters\n",
    "        b -= lr * dl_db\n",
    "        w -= lr * dl_dw\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "\n",
    "- Dampens oscillations by maintaining momentum from previous direction so steps don't zig zag as much, which should help get to minima faster\n",
    "- Beta is commonly set to 0.9, so the parameter update is 90% same direction as previous step and 10% current gradient\n",
    "- Note that previous step includes effect of all prior steps where the most recent ones are exponentially weighted (exponential average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 0\n",
    "w = 0\n",
    "b_step_prev = 0\n",
    "w_step_prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 2.55 (actual 30.00)\n",
      "w = 2.32 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Step size for each parameter\n",
    "        b_step = beta*b_step_prev + (1-beta)*dl_db\n",
    "        w_step = beta*w_step_prev + (1-beta)*dl_dw\n",
    "        b_step_prev = b_step\n",
    "        w_step_prev = w_step\n",
    "        \n",
    "        # Update parameters\n",
    "        b -= lr * b_step\n",
    "        w -= lr * w_step\n",
    "\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "\n",
    "- Root Mean Square Propogation\n",
    "- Dampens oscillations by taking smaller steps when previous steps have been large, and larger steps when previous steps have been small\n",
    "    - Reduce step component perpendicular to direction of minima and increase step component directly towards it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "lr = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 0\n",
    "w = 0\n",
    "b_step_prev = 0\n",
    "w_step_prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 8.53 (actual 30.00)\n",
      "w = 2.37 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Step size for each parameter\n",
    "        b_step = beta*b_step_prev + (1-beta)*dl_db**2\n",
    "        w_step = beta*w_step_prev + (1-beta)*dl_dw**2\n",
    "        b_step_prev = b_step\n",
    "        w_step_prev = w_step\n",
    "                \n",
    "        # Update parameters\n",
    "        b -= lr * dl_db/(b_step**0.5)\n",
    "        w -= lr * dl_dw/(w_step**0.5)\n",
    "        \n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "\n",
    "- Adaptive Moment\n",
    "- Combination of both Momentum and RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_m = 0.9\n",
    "beta_r = 0.999\n",
    "lr = 1.0\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 0\n",
    "w = 0\n",
    "b_m_step_prev = 0\n",
    "w_m_step_prev = 0\n",
    "b_r_step_prev = 0\n",
    "w_r_step_prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 29.97153 (actual 30.00)\n",
      "w = 2.04442 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Momentum step size for each parameter\n",
    "        b_m_step = beta_m*b_m_step_prev + (1-beta_m)*dl_db\n",
    "        w_m_step = beta_m*w_m_step_prev + (1-beta_m)*dl_dw\n",
    "        b_m_step_prev = b_m_step\n",
    "        w_m_step_prev = w_m_step\n",
    "\n",
    "        # RMSProp step size for each parameter\n",
    "        b_r_step = beta_r*b_r_step_prev + (1-beta_r)*dl_db**2\n",
    "        w_r_step = beta_r*w_r_step_prev + (1-beta_r)*dl_dw**2\n",
    "        b_r_step_prev = b_r_step\n",
    "        w_r_step_prev = w_r_step\n",
    "\n",
    "        # Update parameters\n",
    "        b -= lr * b_m_step/(b_r_step**0.5)\n",
    "        w -= lr * w_m_step/(w_r_step**0.5)\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.5f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.5f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate annealing\n",
    "\n",
    "If we train for additional epochs, the parameters values will bounce around the true values. So even though Adam adjusts the step size, applying learning rate annealing (reducing) can help convergence.\n",
    "\n",
    "#### Epsilon\n",
    "\n",
    "Above implementation is missing the $\\epsilon$ term in the parameter update:\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\theta}{\\sqrt{s}+\\epsilon}$$\n",
    "\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\theta$ is the gradient\n",
    "\n",
    "$\\epsilon$ is suggested to be 1e-8 and doesn't usually require tuning\n",
    "\n",
    "#### Bias correction\n",
    "\n",
    "For exponentially weighted averages, the first few values won't have much history. So the This can be corrected by:\n",
    "\n",
    "$$s_t = \\beta s_{t-1} + (1-\\beta) \\theta_t$$\n",
    "\n",
    "$$s_t^{corrected} = \\frac{s_t}{1-\\beta^t}$$\n",
    "\n",
    "Practically speaking, because we usually train for many iterations, there probably isn't a need to apply this correction since it only really affects the first few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- [fast.ai Lesson 5](https://course.fast.ai/videos/?lesson=5)\n",
    "- [Deeplearning.ai RMSprop](https://www.youtube.com/watch?v=_e-LFe_igno)\n",
    "- [Deeplearning.ai Bias correction in exponentially weighted average](https://www.youtube.com/watch?v=lWzo8CajF5s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nikola": {
   "category": "",
   "date": "2019-05-26 19:46:00 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "adaptive-learning-rate-optimisation",
   "tags": "",
   "title": "Adaptive Learning Rate Optimisation",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
