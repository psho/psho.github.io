{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([14,86,28,51,28,29,72,62,84,15,\n",
    "              42,62,47,35,9,38,44,99,13,21,\n",
    "              28,20,8,64,99,70,27,17,8])\n",
    "\n",
    "B = 30\n",
    "W = 2\n",
    "\n",
    "y = B + W*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Loss function is square error:\n",
    "\n",
    "$$loss=(b+wx-y)^2$$\n",
    "\n",
    "Gradients by chain rule:\n",
    "\n",
    "$$F'(g(x)) = f’(g(x))g’(x)$$\n",
    "\n",
    "$$\\frac{dl}{db} = 2(b+wx-y)$$\n",
    "\n",
    "$$\\frac{dl}{dw} = 2(b+wx-y)*x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 1\n",
    "w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 3.73 (actual 30.00)\n",
      "w = 2.56 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "        \n",
    "        # Update parameters\n",
    "        b -= lr * dl_db\n",
    "        w -= lr * dl_dw\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "\n",
    "- Parameter update is 90% the same direction as the previous step and 10% current gradient\n",
    "- Note that previous step includes effect of all prior steps where the most recent ones are exponentially weighted (exponential average)\n",
    "- Dampens oscillations by maintaining momentum from previous direction so steps don't zig zag as much, which should help get to minima faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 1\n",
    "w = 1\n",
    "b_step_prev = 1\n",
    "w_step_prev = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 3.46 (actual 30.00)\n",
      "w = 2.31 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Step size for each parameter\n",
    "        b_step = beta*b_step_prev + (1-beta)*dl_db\n",
    "        w_step = beta*w_step_prev + (1-beta)*dl_dw\n",
    "        b_step_prev = b_step\n",
    "        w_step_prev = w_step\n",
    "        \n",
    "        # Update parameters\n",
    "        b -= lr * b_step\n",
    "        w -= lr * w_step\n",
    "\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "\n",
    "- Root Mean Square Propogation\n",
    "- Dampens oscillations by taking smaller steps when previous steps have been large, and larger steps when previous steps have been small\n",
    "    - Reduce step component perpendicular to direction of minima and increase step component directly towards it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "lr = 0.02\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 1\n",
    "w = 1\n",
    "b_step_prev = 1\n",
    "w_step_prev = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 15.78 (actual 30.00)\n",
      "w = 2.24 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Step size for each parameter\n",
    "        b_step = beta*b_step_prev + (1-beta)*dl_db**2\n",
    "        w_step = beta*w_step_prev + (1-beta)*dl_dw**2\n",
    "        b_step_prev = b_step\n",
    "        w_step_prev = w_step\n",
    "                \n",
    "        # Update parameters\n",
    "        b -= lr/(b_step**0.5) * dl_db\n",
    "        w -= lr/(w_step**0.5) * dl_dw\n",
    "        \n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "\n",
    "- Adaptive Moment\n",
    "- Combination of both Momentum and RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_m = 0.9\n",
    "beta_r = 0.9\n",
    "lr = 1.0\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 1\n",
    "w = 1\n",
    "b_m_step_prev = 1\n",
    "w_m_step_prev = 1\n",
    "b_r_step_prev = 1\n",
    "w_r_step_prev = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 29.06 (actual 30.00)\n",
      "w = 1.39 (actual 2.00)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        dl_db = 2*(y_pred-y[i])\n",
    "        dl_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Momentum step size for each parameter\n",
    "        b_m_step = beta_m*b_m_step_prev + (1-beta_m)*dl_db\n",
    "        w_m_step = beta_m*w_m_step_prev + (1-beta_m)*dl_dw\n",
    "        b_m_step_prev = b_m_step\n",
    "        w_m_step_prev = w_m_step\n",
    "\n",
    "        # RMSProp step size for each parameter\n",
    "        b_r_step = beta_r*b_r_step_prev + (1-beta_r)*dl_db**2\n",
    "        w_r_step = beta_r*w_r_step_prev + (1-beta_r)*dl_dw**2\n",
    "        b_r_step_prev = b_r_step\n",
    "        w_r_step_prev = w_r_step\n",
    "\n",
    "        # Update parameters\n",
    "        b -= lr/(b_r_step**0.5) * b_m_step\n",
    "        w -= lr/(w_r_step**0.5) * w_m_step\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches the true values a lot faster than either Momentum or RMSprop alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam with Annealing\n",
    "\n",
    "Although Adam does scale the step size, we can also apply learning rate annealing to help fine tune the result as it gets closer to the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_m = 0.9\n",
    "beta_r = 0.95\n",
    "epochs = 50\n",
    "\n",
    "# Initial values\n",
    "b = 1\n",
    "w = 1\n",
    "b_m_step_prev = 1\n",
    "w_m_step_prev = 1\n",
    "b_r_step_prev = 1\n",
    "w_r_step_prev = 1\n",
    "\n",
    "lr = 1.0\n",
    "avg_grad_prev = 5500  # Init as value of avg_grad after 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 epochs:\n",
      "b = 30.00 (actual 30.00)\n",
      "w = 2.00 (actual 2.00)\n",
      "lr = 0.00024414\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # Gradient wrt to each parameter\n",
    "        y_pred = b + w*x[i]\n",
    "        de_db = 2*(y_pred-y[i])\n",
    "        de_dw = 2*(y_pred-y[i])*x[i]\n",
    "\n",
    "        # Momentum step size for each parameter\n",
    "        b_m_step = beta_m*b_m_step_prev + (1-beta_m)*de_db\n",
    "        w_m_step = beta_m*w_m_step_prev + (1-beta_m)*de_dw\n",
    "        b_m_step_prev = b_m_step\n",
    "        w_m_step_prev = w_m_step\n",
    "\n",
    "        # RMSProp step size for each parameter\n",
    "        b_r_step = beta_r*b_r_step_prev + (1-beta_r)*de_db**2\n",
    "        w_r_step = beta_r*w_r_step_prev + (1-beta_r)*de_dw**2\n",
    "        b_r_step_prev = b_r_step\n",
    "        w_r_step_prev = w_r_step\n",
    "\n",
    "        # Update parameters\n",
    "        b -= lr/(b_r_step**0.5) * b_m_step\n",
    "        w -= lr/(w_r_step**0.5) * w_m_step\n",
    "        \n",
    "    avg_grad = b_r_step**0.5 + w_r_step**0.5\n",
    "    \n",
    "    if avg_grad/avg_grad_prev > 2:\n",
    "        lr = lr/4\n",
    "    \n",
    "    if avg_grad < avg_grad_prev:\n",
    "        avg_grad_prev = avg_grad\n",
    "        \n",
    "print(\"After %d epochs:\" % epochs)\n",
    "print(\"b = %.2f (actual %.2f)\" % (b,B))\n",
    "print(\"w = %.2f (actual %.2f)\" % (w,W))\n",
    "print(\"lr = %.8f\" % lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller learning rates closer to the minima allows convergence to the true values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://course.fast.ai/videos/?lesson=5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nikola": {
   "category": "",
   "date": "2019-05-26 19:46:00 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "adaptive-learning-rate-optimisation",
   "tags": "",
   "title": "Adaptive Learning Rate Optimisation",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
